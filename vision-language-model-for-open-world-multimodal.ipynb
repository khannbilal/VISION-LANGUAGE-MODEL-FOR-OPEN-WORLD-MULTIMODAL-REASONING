{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1462296,"sourceType":"datasetVersion","datasetId":857191},{"sourceId":3798293,"sourceType":"datasetVersion","datasetId":2264789},{"sourceId":3887986,"sourceType":"datasetVersion","datasetId":2310141},{"sourceId":7289180,"sourceType":"datasetVersion","datasetId":4227284},{"sourceId":13543838,"sourceType":"datasetVersion","datasetId":8601301},{"sourceId":13544505,"sourceType":"datasetVersion","datasetId":8601800},{"sourceId":13544515,"sourceType":"datasetVersion","datasetId":8601808}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"paths = {\n    \"refcoco+\": \"/kaggle/input/refcoco-and-refcoco/refcoco+\",\n    \"refcoco\": \"/kaggle/input/refcoco-and-refcoco/refcoco\",\n    \"vizwiz_annotations\": \"/kaggle/input/vizwiz/Annotations\",\n    \"vizwiz_val\": \"/kaggle/input/vizwiz/val\",\n    \"vizwiz_test\": \"/kaggle/input/vizwiz/test\",\n    \"vizwiz_train\": \"/kaggle/input/vizwiz/train\",\n    \"coco2017\": \"/kaggle/input/coco-2017-dataset/coco2017\",\n    \"vqa_dataset\": \"/kaggle/input/visual-question-answering-computer-vision-nlp/dataset\"\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import BertTokenizerFast\n\ntokenizer = BertTokenizerFast.from_pretrained(\"/kaggle/input/all-files\")\nprint(\"Tokenizer loaded successfully.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --quiet open_clip_torch\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nfrom PIL import Image\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizerFast\n\npaths = {\n    \"refcoco+\": \"/kaggle/input/refcoco-and-refcoco/refcoco+\",\n    \"refcoco\": \"/kaggle/input/refcoco-and-refcoco/refcoco\",\n    \"vizwiz_annotations\": \"/kaggle/input/vizwiz/Annotations\",\n    \"vizwiz_val\": \"/kaggle/input/vizwiz/val\",\n    \"vizwiz_test\": \"/kaggle/input/vizwiz/test\",\n    \"vizwiz_train\": \"/kaggle/input/vizwiz/train\",\n    \"coco2017\": \"/kaggle/input/coco-2017-dataset/coco2017\",\n    \"vqa_dataset\": \"/kaggle/input/visual-question-answering-computer-vision-nlp/dataset\",\n    \"tokenizer\": \"/kaggle/input/all-files\"\n}\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntokenizer = BertTokenizerFast.from_pretrained(paths[\"tokenizer\"])\n\nclass CocoCaptionDataset(Dataset):\n    def __init__(self, image_dir, annotation_file, transform, tokenizer):\n        with open(annotation_file, 'r') as f:\n            self.annotations = json.load(f)['annotations']\n        self.image_dir = image_dir\n        self.transform = transform\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, idx):\n        ann = self.annotations[idx]\n        img_path = os.path.join(self.image_dir, f\"{ann['image_id']:012d}.jpg\")\n        image = Image.open(img_path).convert('RGB')\n        image = self.transform(image)\n        caption = self.tokenizer(\n            ann['caption'],\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        return image, caption.input_ids.squeeze(), caption.attention_mask.squeeze()\n\ncoco_dataset = CocoCaptionDataset(\n    image_dir=os.path.join(paths['coco2017'], 'train2017'),\n    annotation_file=os.path.join(paths['coco2017'], 'annotations', 'captions_train2017.json'),\n    transform=transform,\n    tokenizer=tokenizer\n)\n\ncoco_loader = DataLoader(coco_dataset, batch_size=8, shuffle=True)\n\nprint(f\"COCO dataset samples: {len(coco_dataset)}\")\nsample_img, sample_ids, sample_mask = next(iter(coco_loader))\nprint(f\"Sample batch - images: {sample_img.shape}, token_ids: {sample_ids.shape}, attention_mask: {sample_mask.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport torch\nfrom PIL import Image\nfrom torchvision import transforms\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizerFast\nfrom open_clip import create_model_and_transforms\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\npaths = {\n    \"coco2017\": \"/kaggle/input/coco-2017-dataset/coco2017\",\n    \"tokenizer\": \"/kaggle/input/all-files\"  # or wherever your tokenizer files are stored\n}\n\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\nmodel, _, preprocess_train = create_model_and_transforms(\n    model_name=\"ViT-B-32\",\n    pretrained=\"laion2b_s34b_b79k\",\n    device=device\n)\nmodel = model.to(device)\n\nfor param in model.visual.parameters():\n    param.requires_grad = False\nfor param in model.transformer.parameters():\n    param.requires_grad = False\n\ntokenizer = BertTokenizerFast.from_pretrained(paths[\"tokenizer\"])\n\nclass CocoCaptionDataset(Dataset):\n    def __init__(self, image_dir, annotation_file, transform, tokenizer, max_length=77):\n        with open(annotation_file, 'r') as f:\n            self.annotations = json.load(f)['annotations']\n        self.image_dir = image_dir\n        self.transform = transform\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, idx):\n        ann = self.annotations[idx]\n        img_path = os.path.join(self.image_dir, f\"{ann['image_id']:012d}.jpg\")\n        image = Image.open(img_path).convert('RGB')\n        image = self.transform(image)\n        encoded = self.tokenizer(\n            ann['caption'],\n            padding='max_length',\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n        return image, encoded.input_ids.squeeze(0), encoded.attention_mask.squeeze(0)\n\ndef collate_fn(batch):\n    images, token_ids, attention_masks = zip(*batch)\n    images = torch.stack(images)\n    token_ids = torch.stack(token_ids)\n    attention_masks = torch.stack(attention_masks)\n    return images, token_ids, attention_masks\n\ncoco_dataset = CocoCaptionDataset(\n    image_dir=os.path.join(paths['coco2017'], 'train2017'),\n    annotation_file=os.path.join(paths['coco2017'], 'annotations', 'captions_train2017.json'),\n    transform=transform,\n    tokenizer=tokenizer\n)\n\ncoco_loader = DataLoader(coco_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n\noptimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5)\ncriterion = nn.CosineEmbeddingLoss()\n\nmodel.train()\nfor images, token_ids, attention_masks in coco_loader:\n    images = images.to(device)\n    token_ids = token_ids.to(device)\n\n    image_features = model.encode_image(images)\n    text_features = model.encode_text(token_ids)\n\n    labels = torch.ones(image_features.size(0)).to(device)\n    loss = criterion(image_features, text_features, labels)\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    print(f\"Batch loss: {loss.item()}\")\n    break\n\nwith torch.no_grad():\n    cosine_sim = nn.functional.cosine_similarity(image_features, text_features)\n    print(f\"Cosine similarity sample: {cosine_sim[:5]}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom PIL import Image\nfrom torchvision import transforms\nfrom open_clip import create_model_and_transforms\nfrom peft import get_peft_model, LoraConfig, TaskType\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel, _, preprocess = create_model_and_transforms(\n    model_name=\"ViT-B-32\",\n    pretrained=\"laion2b_s34b_b79k\",\n    device=device\n)\nmodel = model.to(device)\n\nclass VisualCompressor(nn.Module):\n    def __init__(self, input_dim=512, output_dim=256, num_tokens=8):\n        super().__init__()\n        self.proj = nn.Linear(input_dim, output_dim)\n        self.pool = nn.AdaptiveAvgPool1d(num_tokens)\n\n    def forward(self, *args, **kwargs):\n        x = kwargs.get(\"input_ids\", None)\n        if x is None:\n            x = kwargs.get(\"inputs_embeds\", None)\n        if x is None:\n            raise ValueError(\"Input tensor x is required (passed as input_ids or inputs_embeds)\")\n        x = self.proj(x)\n        x = x.transpose(1, 2)\n        x = self.pool(x)\n        return x.transpose(1, 2)\n\ncompressor = VisualCompressor().to(device)\n\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=32,\n    target_modules=[\"proj\"],\n    lora_dropout=0.1,\n    bias=\"none\",\n    task_type=TaskType.FEATURE_EXTRACTION\n)\n\ncompressor = get_peft_model(compressor, lora_config)\ncompressor.print_trainable_parameters()\n\nimage = Image.open(\"/kaggle/input/coco-2017-dataset/coco2017/train2017/000000000009.jpg\").convert(\"RGB\")\nimage_tensor = preprocess(image).unsqueeze(0).to(device)\n\nwith torch.no_grad():\n    image_features = model.encode_image(image_tensor)\n    image_tokens = compressor(image_features.unsqueeze(1))\n\nprint(\"Compressed visual tokens:\", image_tokens.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nimport json\nfrom PIL import Image\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\nfrom open_clip import create_model_and_transforms\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\npaths = {\n    \"coco2017\": \"/kaggle/input/coco-2017-dataset/coco2017\",\n    \"vqa_dataset\": \"/kaggle/input/visual-question-answering-computer-vision-nlp/dataset\",\n    \"refcoco\": \"/kaggle/input/refcoco-and-refcoco/refcoco\",\n    \"tokenizer\": \"/kaggle/input/gpt-2-tokens\",\n    \"decoder\": \"/kaggle/input/gpt-2-model\"\n}\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n])\n\nmodel, _, preprocess = create_model_and_transforms(\n    model_name=\"ViT-B-32\",\n    pretrained=\"laion2b_s34b_b79k\",\n    device=device\n)\nmodel = model.to(device)\nfor param in model.visual.parameters():\n    param.requires_grad = False\nfor param in model.transformer.parameters():\n    param.requires_grad = False\n\ntokenizer = GPT2Tokenizer.from_pretrained(paths[\"tokenizer\"], local_files_only=True)\ntokenizer.pad_token = tokenizer.eos_token\ndecoder = GPT2LMHeadModel.from_pretrained(paths[\"decoder\"], local_files_only=True).to(device)\n\nclass VisualCompressor(nn.Module):\n    def __init__(self, input_dim=512, compressed_dim=256, output_dim=768, num_tokens=8):\n        super().__init__()\n        self.proj = nn.Linear(input_dim, compressed_dim)\n        self.pool = nn.AdaptiveAvgPool1d(num_tokens)\n        self.expand = nn.Linear(compressed_dim, output_dim)\n    def forward(self, x):\n        x = self.proj(x)\n        x = x.transpose(1,2)\n        x = self.pool(x)\n        x = x.transpose(1,2)\n        x = self.expand(x)\n        return x\n\ncompressor = VisualCompressor().to(device)\nfor param in compressor.parameters():\n    param.requires_grad = False\nfor name, param in compressor.named_parameters():\n    if \"proj.weight\" in name or \"proj.bias\" in name:\n        param.requires_grad = True\n\ntrainable = sum(p.numel() for p in compressor.parameters() if p.requires_grad)\ntotal = sum(p.numel() for p in compressor.parameters())\nprint(f\"trainable params: {trainable} || all params: {total} || trainable%: {100 * trainable / total:.4f}\")\n\nclass MultiTaskDataset(Dataset):\n    def __init__(self, coco_file, vqa_file, refcoco_file, transform, tokenizer, max_len=128):\n        with open(coco_file) as f:\n            self.coco = json.load(f)['annotations']\n        self.vqa = pd.read_csv(vqa_file).to_dict(orient='records')\n        with open(refcoco_file) as f:\n            self.refcoco = json.load(f)['annotations']\n        self.transform = transform\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    def __len__(self):\n        return len(self.coco) + len(self.vqa) + len(self.refcoco)\n    def __getitem__(self, idx):\n        if idx < len(self.coco):\n            ann = self.coco[idx]\n            task = 'caption'\n            img_path = os.path.join(paths['coco2017'], 'train2017', f\"{ann['image_id']:012d}.jpg\")\n            text = ann.get('caption', '')\n        elif idx < len(self.coco)+len(self.vqa):\n            ann = self.vqa[idx - len(self.coco)]\n            task = 'vqa'\n            img_path = os.path.join(paths['vqa_dataset'], 'images', f\"{ann['image_id']}.png\")\n            text = ann.get('question', '')\n        else:\n            ann = self.refcoco[idx - len(self.coco) - len(self.vqa)]\n            task = 'grounding'\n            img_path = os.path.join(paths['coco2017'], 'train2017', f\"{ann['image_id']:012d}.jpg\")\n            text = ann.get('caption', '') or ann.get('question', '') or ann.get('phrase', '')\n        text = text.strip()\n        if len(text) == 0:\n            text = \"unknown\"\n        image = Image.open(img_path).convert(\"RGB\")\n        image = self.transform(image)\n        enc = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_len, return_tensors='pt')\n        return image, enc.input_ids.squeeze(0), enc.attention_mask.squeeze(0), task\n\ndef collate_fn(batch):\n    images, token_ids, attention_masks, tasks = zip(*batch)\n    images = torch.stack(images)\n    token_ids = torch.stack(token_ids)\n    attention_masks = torch.stack(attention_masks)\n    return images, token_ids, attention_masks, tasks\n\ndataset = MultiTaskDataset(\n    coco_file=os.path.join(paths['coco2017'],'annotations','captions_train2017.json'),\n    vqa_file=os.path.join(paths['vqa_dataset'],'data_train.csv'),\n    refcoco_file=os.path.join(paths['refcoco'],'instances.json'),\n    transform=transform,\n    tokenizer=tokenizer\n)\nloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n\noptimizer = optim.AdamW(list(model.parameters()) + list(decoder.parameters()) + list(compressor.parameters()), lr=5e-5)\nloss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n\nmodel.train()\ndecoder.train()\ncompressor.train()\n\nfor images, token_ids, attention_masks, tasks in loader:\n    images = images.to(device)\n    token_ids = token_ids.to(device)\n    attention_masks = attention_masks.to(device)\n\n    image_features = model.encode_image(images)\n    image_tokens = compressor(image_features.unsqueeze(1))\n\n    text_embeds = decoder.transformer.wte(token_ids)\n    inputs_embeds = torch.cat([image_tokens, text_embeds], dim=1)\n\n    visual_mask = torch.ones(image_tokens.size(0), image_tokens.size(1)).to(device)\n    attention_mask = torch.cat([visual_mask, attention_masks], dim=1)\n\n    visual_labels = torch.full((image_tokens.size(0), image_tokens.size(1)), -100).to(device)\n    labels = torch.cat([visual_labels, token_ids], dim=1)\n\n    outputs = decoder(inputs_embeds=inputs_embeds, attention_mask=attention_mask, labels=labels)\n    loss = outputs.loss\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    print(f\"Batch loss: {loss.item()}\")\n    break\n\nwith torch.no_grad():\n    text_mean = decoder.transformer.wte(token_ids).mean(dim=1)  # [4, 768]\n    image_proj = nn.Linear(512, 768).to(device)\n    image_proj.eval()\n    image_aligned = image_proj(image_features)  # [4, 768]\n    cosine_sim = nn.functional.cosine_similarity(image_aligned, text_mean, dim=-1)\n    print(f\"Cosine similarity sample: {cosine_sim[:5]}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!sudo apt update\n!sudo apt install -y git wget unzip ffmpeg\n!git clone https://github.com/facebookresearch/ov-seg.git\n%cd ov-seg\n!pip install -r requirements.txt\n!pip install -e .\n!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118\n!pip install git+https://github.com/facebookresearch/detectron2.git@main\n!mkdir -p checkpoints\n!wget -O checkpoints/refcocog_clip.pth https://dl.fbaipublicfiles.com/ovseg/refcocog_clip.pth","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install git+https://github.com/<username>/ovseg.git\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torchvision.transforms as T\nfrom PIL import Image\nfrom ovseg.models import build_model\nfrom ovseg.utils import load_config, load_checkpoint\nfrom ovseg.evaluation import compute_iou\nimport json\nimport os\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Step 1: Load Mask2Former + CLIP model\nconfig = load_config(\"configs/refcocog_clip.yaml\")\nmodel = build_model(config).to(device)\nmodel.eval()\nload_checkpoint(model, \"checkpoints/refcocog_clip.pth\")\n\n# Step 2: Load image and text query\nimage_path = \"refcocog/images/image1174.jpg\"\nimage = Image.open(image_path).convert(\"RGB\")\ntransform = T.Compose([\n    T.Resize((512, 512)),\n    T.ToTensor(),\n    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\nimage_tensor = transform(image).unsqueeze(0).to(device)\ntext_query = \"the curtain to the right of the bed\"\n\n# Step 3: Run inference\nwith torch.no_grad():\n    outputs = model(image_tensor, text_query)\n\nmasks = outputs[\"pred_masks\"]  # [N, H, W]\nboxes = outputs[\"pred_boxes\"]  # [N, 4]\nscores = outputs[\"scores\"]     # [N]\n\n# Step 4: Load ground truth and compute IoU\ndef load_gt_boxes(json_path, image_id):\n    with open(json_path) as f:\n        data = json.load(f)\n    return [ann[\"bbox\"] for ann in data[\"annotations\"] if ann[\"image_id\"] == image_id]\n\ngt_boxes = load_gt_boxes(\"refcocog/annotations/instances.json\", image_id=1174)\npred_boxes = boxes.cpu().numpy()\nious = compute_iou(pred_boxes, gt_boxes)\nmean_iou = ious.mean()\n\nprint(f\"Detected {len(pred_boxes)} regions\")\nprint(f\"Mean IoU on RefCOCOg: {mean_iou:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torchvision.transforms as T\nimport torchvision.transforms.functional as TF\nfrom PIL import Image, ImageEnhance, ImageFilter\nimport random\nimport numpy as np\n\ndef apply_occlusion(img, box_size=80):\n    w, h = img.size\n    x = random.randint(0, w - box_size)\n    y = random.randint(0, h - box_size)\n    img.paste((0, 0, 0), [x, y, x + box_size, y + box_size])\n    return img\n\ndef apply_blur(img, radius=3):\n    return img.filter(ImageFilter.GaussianBlur(radius))\n\ndef apply_lighting(img, factor=0.5):\n    enhancer = ImageEnhance.Brightness(img)\n    return enhancer.enhance(factor)\n\ndef apply_domain_shift(img_path):\n    # VizWiz image loader\n    return Image.open(img_path).convert(\"RGB\")\n\ndef evaluate_model(model, image, text, transform, tokenizer, task=\"vqa\"):\n    image_tensor = transform(image).unsqueeze(0).to(device)\n    enc = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n    token_ids = enc.input_ids.to(device)\n    attention_mask = enc.attention_mask.to(device)\n\n    with torch.no_grad():\n        image_features = model.encode_image(image_tensor)\n        image_tokens = compressor(image_features.unsqueeze(1))\n        text_embeds = decoder.transformer.wte(token_ids)\n        inputs_embeds = torch.cat([image_tokens, text_embeds], dim=1)\n        visual_mask = torch.ones(image_tokens.size(0), image_tokens.size(1)).to(device)\n        full_mask = torch.cat([visual_mask, attention_mask], dim=1)\n        visual_labels = torch.full((image_tokens.size(0), image_tokens.size(1)), -100).to(device)\n        labels = torch.cat([visual_labels, token_ids], dim=1)\n        outputs = decoder(inputs_embeds=inputs_embeds, attention_mask=full_mask, labels=labels)\n        return outputs.loss.item()\n\nimage_path = \"refcocog/images/image1174.jpg\"\ntext_query = \"the curtain to the right of the bed\"\noriginal = Image.open(image_path).convert(\"RGB\")\n\nperturbations = {\n    \"original\": original,\n    \"occlusion\": apply_occlusion(original.copy()),\n    \"blur\": apply_blur(original.copy()),\n    \"lighting\": apply_lighting(original.copy()),\n    \"vizwiz\": apply_domain_shift(\"vizwiz/image_0001.jpg\")\n}\n\nlosses = {}\nfor name, img in perturbations.items():\n    loss = evaluate_model(model, img, text_query, transform, tokenizer, task=\"vqa\")\n    losses[name] = loss\n    print(f\"{name} loss: {loss:.4f}\")\n\nbase = losses[\"original\"]\nfor k, v in losses.items():\n    if k != \"original\":\n        drop = v - base\n        print(f\"Î” Loss ({k}): {drop:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport time\nimport numpy as np\nfrom sklearn.metrics import average_precision_score\n\ndef evaluate_vqa(model, dataset, tokenizer):\n    correct = 0\n    total = 0\n    for image, question, answer in dataset:\n        image = image.to(device)\n        enc = tokenizer(question, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n        token_ids = enc.input_ids.to(device)\n        attention_mask = enc.attention_mask.to(device)\n\n        with torch.no_grad():\n            image_features = model.encode_image(image.unsqueeze(0))\n            image_tokens = compressor(image_features.unsqueeze(1))\n            text_embeds = decoder.transformer.wte(token_ids)\n            inputs_embeds = torch.cat([image_tokens, text_embeds], dim=1)\n            visual_mask = torch.ones(image_tokens.size(0), image_tokens.size(1)).to(device)\n            full_mask = torch.cat([visual_mask, attention_mask], dim=1)\n            visual_labels = torch.full((image_tokens.size(0), image_tokens.size(1)), -100).to(device)\n            labels = torch.cat([visual_labels, token_ids], dim=1)\n            outputs = decoder(inputs_embeds=inputs_embeds, attention_mask=full_mask, labels=labels)\n            pred = tokenizer.decode(outputs.logits.argmax(-1)[0], skip_special_tokens=True)\n            if pred.strip().lower() == answer.strip().lower():\n                correct += 1\n            total += 1\n    return 100 * correct / total\n\ndef compute_iou(boxA, boxB):\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n    interArea = max(0, xB - xA) * max(0, yB - yA)\n    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n    iou = interArea / float(boxAArea + boxBArea - interArea)\n    return iou\n\ndef evaluate_grounding(model, grounding_dataset):\n    ious = []\n    for image, phrase, gt_box in grounding_dataset:\n        image = image.to(device)\n        with torch.no_grad():\n            outputs = model(image.unsqueeze(0), phrase)\n            pred_box = outputs[\"pred_boxes\"][0].cpu().numpy()\n            iou = compute_iou(pred_box, gt_box)\n            ious.append(iou)\n    return np.mean(ious)\n\ndef evaluate_retrieval(model, retrieval_dataset, k=10):\n    image_features = []\n    text_features = []\n    for image, text in retrieval_dataset:\n        image = image.to(device)\n        with torch.no_grad():\n            image_feat = model.encode_image(image.unsqueeze(0)).cpu().numpy()\n            text_enc = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n            text_feat = decoder.transformer.wte(text_enc.input_ids.to(device)).mean(dim=1).cpu().numpy()\n        image_features.append(image_feat)\n        text_features.append(text_feat)\n\n    image_features = np.vstack(image_features)\n    text_features = np.vstack(text_features)\n    sims = np.dot(text_features, image_features.T)\n\n    mAP = average_precision_score(np.eye(len(sims)), sims)\n    recall_at_k = np.mean([int(i in sims[i].argsort()[-k:]) for i in range(len(sims))])\n    return mAP, recall_at_k\n\ndef evaluate_efficiency(model, image, text):\n    image = image.to(device)\n    enc = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n    token_ids = enc.input_ids.to(device)\n    attention_mask = enc.attention_mask.to(device)\n\n    torch.cuda.reset_peak_memory_stats()\n    start = time.time()\n    with torch.no_grad():\n        image_features = model.encode_image(image.unsqueeze(0))\n        image_tokens = compressor(image_features.unsqueeze(1))\n        text_embeds = decoder.transformer.wte(token_ids)\n        inputs_embeds = torch.cat([image_tokens, text_embeds], dim=1)\n        visual_mask = torch.ones(image_tokens.size(0), image_tokens.size(1)).to(device)\n        full_mask = torch.cat([visual_mask, attention_mask], dim=1)\n        decoder(inputs_embeds=inputs_embeds, attention_mask=full_mask)\n    end = time.time()\n    latency = end - start\n    memory = torch.cuda.max_memory_allocated() / 1e6\n    return latency, memory\n\nvqa_acc = evaluate_vqa(model, vqa_dataset, tokenizer)\niou_score = evaluate_grounding(model, refcocog_dataset)\nmAP, recall_k = evaluate_retrieval(model, retrieval_dataset)\nlatency, memory = evaluate_efficiency(model, sample_image, sample_text)\n\nprint(f\"VQA Accuracy: {vqa_acc:.2f}%\")\nprint(f\"RefCOCOg IoU: {iou_score:.4f}\")\nprint(f\"Retrieval mAP: {mAP:.4f}, Recall@10: {recall_k:.4f}\")\nprint(f\"Inference latency: {latency:.2f}s, Memory usage: {memory:.2f}MB\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}